# =============================
# [1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ëª¨ë“ˆ ì„í¬íŠ¸
# =============================
import torch
import torch.nn as nn
from transformers import AutoModel, get_linear_schedule_with_warmup
from tqdm import tqdm
from classifier import get_dataloaders
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, precision_recall_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import os

# =============================
# [2] í‰ê°€ ì§€í‘œ ë° ì‹œê°í™” í•¨ìˆ˜ ì •ì˜-ê° ì—í¬í¬ë§ˆë‹¤ ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆê²Œ ì„¤ì •
# =============================
def evaluate_competition(y_true, y_pred, y_probs=None, class_names=None):
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='macro')
    precision = precision_score(y_true, y_pred, average='macro')
    recall = recall_score(y_true, y_pred, average='macro')

    print(f"Accuracy: {acc:.4f}")                    #Accuracyê³„ì‚°
    print(f"Precision (macro): {precision:.4f}")     #Precisionê³„ì‚°
    print(f"Recall (macro): {recall:.4f}")           #Recallê³„ì‚°
    print(f"F1-score (macro): {f1:.4f}")             #F1-scoreê³„ì‚°

# =============================
# [3] ëª¨ë¸ ì •ì˜
# =============================
class DebertaClassifier(nn.Module):   #PTorchì˜ nn.moduleìƒì†, ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ í´ë˜ìŠ¤ ë§Œë“¬:
    def __init__(self, pretrained_model="microsoft/deberta-v3-base", hidden_size=768, dropout_rate=0.3, num_classes=4):   #ê¸°ë³¸ê°’ deberta-v3-baseì‚¬ìš©,ì¶œë ¥ì°¨ì›:4(í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„¤ì •)
        super().__init__()
        self.encoder = AutoModel.from_pretrained(pretrained_model)      #AutoModelì„ ì‚¬ìš©í•´ DeBERTa-v3ê¸°ë°˜ Transformerì¸ì½”ë” ë¶ˆëŸ¬ì˜¤ê¸°, ì¶œë ¥ì€ hidden stateì „ì²´(batch_size X seq_len X hidden_dim), ì—¬ê¸°ì„œ [CLS]í† í°ì— í•´ë‹¹í•˜ëŠ” ì²« í† í° ì¶œë ¥ë§Œ ì‚¬ìš©
        self.dropout = nn.Dropout(dropout_rate)                         #ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ dropoutì ìš©, classification headì— ë“¤ì–´ê°€ê¸° ì „ [CLS]ë²¡í„°ì— ì ìš©
        self.classifier = nn.Linear(hidden_size, num_classes)           #hidden stateí¬ê¸°(768)-> í´ë˜ìŠ¤ 4ë¡œ ë§¤í•‘

    def forward(self, input_ids, attention_mask, labels=None):           #ìˆœì „íŒŒ ì§„í–‰forward()
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)  #Transformer ì¸ì½”ë” ì‹¤í–‰, outputs.last_hidden_stateëŠ” ì „ì²´ ì‹œí€€ìŠ¤ì˜ ì¶œë ¥(batch, seq_len, hidden)
        cls_output = self.dropout(outputs.last_hidden_state[:, 0, :])               #ì‹œí€€ìŠ¤ ì²« ë²ˆì§¸ í† í° [:,0,:]ì„ ì¶”ì¶œ-> [CLS]ì—­í•  ê·¸ í›„ dropout ì ìš©
        logits = self.classifier(cls_output)                                        #ê° í´ë˜ìŠ¤ì— ëŒ€í•œ logit ì ìˆ˜ (softmax ì „ë‹¨ê³„)
        
        if labels is not None:                                                     #í•™ìŠµ ì‹œ: ì†ì‹¤ ê³„ì‚°
            loss_fct = nn.CrossEntropyLoss()                                       #í•™ìŠµ ë‹¨ê³„: ì •ë‹µ labelsê°€ ì£¼ì–´ì¡Œë‹¤ë©´ CrossEntropyLossë¡œ ì†ì‹¤ê³„ì‚°
            loss = loss_fct(logits, labels)                                        #(loss, logits)ë°˜í™˜í•˜ì—¬ í•™ìŠµ ë£¨í”„ì—ì„œ ì‚¬ìš©
            return loss, logits
        else:
            return logits                                                           #í‰ê°€/ì¶”ë¡  ì‹œì—ëŠ” ì†ì‹¤ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ logitsë§Œ ë°˜í™˜


# =============================
# [4] í•™ìŠµ ë£¨í”„
# =============================

def train_model(train_loader, val_loader, num_epochs=32, lr=2e-5, weight_decay=0.01, patience=100):
    #í•™ìŠµí™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ì¤€ë¹„
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  #í•™ìŠµí™˜ê²½ì˜ gpuê°€ìš©ì—¬ë¶€ íŒë‹¨í•˜ì—¬ ì—°ì‚° ë””ë°”ì´ìŠ¤ ì„¤ì •
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    model = DebertaClassifier().to(device)

    if os.path.exists("best_model.pt"):                           #ê¸°ì¡´ì— ì €ì¥ëœ ìµœì  ê°€ì¤‘ì¹˜ íŒŒì¼(best_model.pt)ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°, í•´ë‹¹ ëª¨ë¸ì„ ë¡œë”©í•˜ì—¬ ì´ì–´ì„œ í•™ìŠµê°€ëŠ¥
        print("ê¸°ì¡´ best_model.pt ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        model.load_state_dict(torch.load("best_model.pt"))
    else:                                                          #ê·¸ ì™¸ì˜ ê²½ìš° ìƒˆë¡œìš´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ í•™ìŠµ ì‹œì‘
        print("ìƒˆë¡œìš´ ëª¨ë¸ë¡œ í•™ìŠµ ì‹œì‘")
    
    #Optimizerë° í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)     #ìµœì í™” í•¨ìˆ˜ AdamWì‚¬ìš©,í•™ìŠµë¥ :2e-5, weight decay:0.01
    total_steps = len(train_loader) * num_epochs                                              
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)  #Warmupì„ í¬í•¨í•œ linear schedulerë¥¼ ì ìš©í•˜ì—¬ ì „ì²´ ìŠ¤í…Œìˆ˜ì— ë¹„ë¡€í•˜ì—¬ learning rateë¥¼ì ì§„ì ìœ¼ë¡œ ì¡°ì •

    best_f1 = 0
    no_improve = 0
    class_names = ["HF", "HR", "MF", "MR"]
    
    #epochê¸°ë°˜ ë°˜ë³µí•™ìŠµ ì§„í–‰-ê° Epochë§ˆë‹¤ train_loaderë¥¼ í†µí•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ë¸ì„ í•™ìŠµ. í•™ìŠµ ì‹œ ê° ë°°ì¹˜ë§ˆë‹¤ gradient ì´ˆê¸°í™” â†’ forward ì—°ì‚° â†’ loss ê³„ì‚° â†’ backward â†’ gradient clipping â†’ optimizer ì—…ë°ì´íŠ¸ â†’ scheduler ì—…ë°ì´íŠ¸ ìˆœì„œë¡œ ì§„í–‰.
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            loss, _ = model(input_ids, attention_mask, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        tqdm.write(f"Epoch {epoch+1} Loss: {avg_loss:.4f}")

        # epochì¢…ë£Œí›„ validation í‰ê°€ìˆ˜í–‰-ê° Epochì´ ì¢…ë£Œëœ í›„, validation ë°ì´í„°ì…‹(val_loader)ì„ í†µí•´ í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€. ì˜ˆì¸¡ëœ ê²°ê³¼ë¡œë¶€í„° Macro F1-score, Accuracy, Precision, Recall ë“±ì˜ ì„±ëŠ¥ ì§€í‘œë¥¼ ì¶œë ¥.

        model.eval()
        all_preds, all_labels, all_probs = [], [], []
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                logits = model(input_ids, attention_mask)
                preds = torch.argmax(logits, dim=1)
                probs = torch.softmax(logits, dim=1)
                all_preds.extend(preds.cpu().tolist())
                all_labels.extend(labels.cpu().tolist())
                all_probs.append(probs.cpu().numpy())

        f1 = f1_score(all_labels, all_preds, average='macro')
        print(f"\n Epoch {epoch+1} | Macro F1: {f1:.4f}")
        evaluate_competition(all_labels, all_preds, np.vstack(all_probs), class_names)

        #best model ì €ì¥ ë¡œì§-í˜„ì¬ Epochì—ì„œ ê³„ì‚°ëœ macro F1-scoreê°€ ì´ì „ê¹Œì§€ì˜ ìµœê³  ì„±ëŠ¥ë³´ë‹¤ ìš°ìˆ˜í•  ê²½ìš°, í•´ë‹¹ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ best_model.ptë¡œ ì €ì¥. ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ no_improve ì¹´ìš´í„°ë¥¼ ì¦ê°€ì‹œì¼œ Early Stoppingì„ ìœ„í•œ ì¤€ë¹„. ê¸°ì¤€ê°’:100
        if f1 > best_f1:
            best_f1 = f1
            no_improve = 0
            #torch.save(model.state_dict(), "best_model.pt")
            torch.save(model.state_dict(), "best_model_more train.pt")
        else:
            no_improve += 1
            if no_improve >= patience:
                print("â¹ï¸ Early stopping!")
                break
    #ìµœì¢…ë°˜í™˜-ìµœì¢…ì ìœ¼ë¡œ í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•˜ë©°, ì´í›„ ë³„ë„ í‰ê°€ ëª¨ë“ˆ(analysis.py)ì—ì„œ "best_model.pt"ê²½ìš°ì˜ validation ë° test ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ë¥¼ ìˆ˜í–‰.
    return model

"""
# =============================
# [5] ì¶”ë¡ 
# =============================
def inference(model, test_loader, save_path="submission.csv"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    preds = []
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            logits = model(input_ids, attention_mask)
            pred = torch.argmax(logits, dim=1)
            preds.extend(pred.cpu().tolist())

    df = pd.DataFrame({"id": list(range(len(preds))), "label": preds})
    df.to_csv(save_path, index=False)
    print(f"ğŸ“¦ submission saved to {save_path}")
"""

# =============================
# [6] ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# =============================
if __name__ == "__main__":
    train_loader, val_loader, test_loader = get_dataloaders()
    model = train_model(train_loader, val_loader)
    model.load_state_dict(torch.load("best_model.pt"))
    #inference(model, test_loader)
